apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: taiji-alerts
  namespace: monitoring
  labels:
    app: kube-prometheus-stack
    release: kps
spec:
  groups:
    - name: taiji.critical
      interval: 1m
      rules:
        # ===== CRITICAL: Service Down (HIGH PRIORITY) =====

        # All instances down - PAGE IMMEDIATELY
        - alert: TaijiCompleteOutage
          expr: |
            (count(up{job="taiji",environment="production"} == 1) or vector(0)) == 0
          for: 2m
          labels:
            severity: critical
            service: taiji
            team: platform
          annotations:
            summary: "ðŸš¨ CRITICAL: ALL Taiji instances are DOWN"
            description: "Complete service outage. Zero healthy instances detected."
            action: "Check Kubernetes pods, logs, and dependencies immediately."

        # Single instance down for a while - lower priority
        - alert: TaijiInstanceDown
          expr: |
            up{job="taiji",environment="production"} == 0
          for: 10m
          labels:
            severity: warning
            service: taiji
            team: platform
          annotations:
            summary: "Taiji instance {{ $labels.instance }} is down"
            description: "Instance has been down for 10+ minutes. Auto-recovery expected."

        # ===== CRITICAL: Sustained High Error Rates (AGGREGATED) =====

        # Overall service degradation - aggregate across all subdomains
        - alert: TaijiServiceDegraded
          expr: |
            (
              sum(rate(proxy_requests_total{job="taiji",environment="production",status_code=~"5.."}[10m]))
              /
              sum(rate(proxy_requests_total{job="taiji",environment="production"}[10m]))
            ) > 0.10
          for: 15m
          labels:
            severity: critical
            service: taiji
            team: platform
          annotations:
            summary: "ðŸš¨ Taiji service degraded - {{ $value | humanizePercentage }} error rate"
            description: "Overall 5xx error rate is {{ $value | humanizePercentage }} for 15+ minutes. Multiple backends may be affected."
            action: "Check backend health, recent deployments, and infrastructure."

        # ===== WARNING: Elevated Error Rates (GROUPED BY SUBDOMAIN) =====

        # Per-subdomain errors - but only alert on sustained issues
        - alert: TaijiSubdomainDegraded
          expr: |
            (
              sum by (subdomain) (rate(proxy_requests_total{job="taiji",environment="production",status_code=~"5.."}[15m]))
              /
              sum by (subdomain) (rate(proxy_requests_total{job="taiji",environment="production"}[15m]))
            ) > 0.20
          for: 20m
          labels:
            severity: warning
            service: taiji
            team: platform
          annotations:
            summary: "Subdomain {{ $labels.subdomain }} degraded - {{ $value | humanizePercentage }} errors"
            description: "Subdomain has {{ $value | humanizePercentage }} 5xx error rate for 20+ minutes."
            action: "Check backend for {{ $labels.subdomain }}."

        # ===== WARNING: Redis Issues (AGGREGATED) =====

        - alert: TaijiRateLimitingDegraded
          expr: |
            sum(rate(proxy_ratelimit_redis_errors_total{job="taiji",environment="production"}[5m])) > 5
          for: 10m
          labels:
            severity: warning
            service: taiji
            team: platform
          annotations:
            summary: "Taiji rate limiting degraded - Redis errors"
            description: "Rate limiting experiencing {{ printf \"%.1f\" $value }} Redis errors/sec for 10+ minutes. Service failing open."
            action: "Check Redis connectivity and health."

        # ===== WARNING: Rate Limiting Abuse (DAILY SUMMARY) =====

        # High blocking rate - potential DDoS/abuse
        - alert: TaijiHighRateLimitBlocking
          expr: |
            (
              sum by (subdomain) (rate(proxy_ratelimit_requests_total{job="taiji",environment="production",action="blocked"}[1h]))
              /
              sum by (subdomain) (rate(proxy_ratelimit_requests_total{job="taiji",environment="production"}[1h]))
            ) > 0.50
          for: 1h
          labels:
            severity: warning
            service: taiji
            team: platform
          annotations:
            summary: "Subdomain {{ $labels.subdomain }} blocking {{ $value | humanizePercentage }} of requests"
            description: "Over 50% of requests blocked for 1+ hour. Possible DDoS or misconfigured limit."
            action: "Review rate limits and check for abuse patterns."

        # ===== WARNING: Taiji is the Bottleneck =====

        # Proxy overhead percentage too high relative to backend time
        # Only alert when overhead is a significant portion of total latency
        - alert: TaijiProxyOverheadPercentageHigh
          expr: |
            (
              sum(rate(proxy_overhead_duration_seconds_sum{job="taiji",environment="production"}[15m]))
              /
              sum(rate(proxy_request_duration_seconds_sum{job="taiji",environment="production"}[15m]))
            ) > 0.30
          for: 30m
          labels:
            severity: warning
            service: taiji
            team: platform
          annotations:
            summary: "Taiji overhead is {{ $value | humanizePercentage }} of total latency"
            description: "Proxy overhead represents {{ $value | humanizePercentage }} of total request time for 30+ minutes (target: <30%). Taiji may be a bottleneck."
            action: "Review resource limits, check for CPU throttling, and consider scaling up."

        # ===== INFO: Configuration Issues =====

        - alert: TaijiConfigReloadFailing
          expr: |
            rate(proxy_csv_reload_errors_total{job="taiji",environment="production"}[10m]) > 0
          for: 15m
          labels:
            severity: warning
            service: taiji
            team: platform
          annotations:
            summary: "Taiji configuration reload failing"
            description: "YAML reload errors detected for 15+ minutes."
            action: "Check ConfigMap/YAML file syntax and mount."

        # ===== CRITICAL: All Backends Down =====

        # All backends unhealthy for a service - critical impact
        - alert: TaijiAllBackendsUnhealthy
          expr: |
            sum by (subdomain) (rate(proxy_all_backends_unhealthy_total{job="taiji",environment="production"}[5m])) > 0
          for: 10m
          labels:
            severity: critical
            service: taiji
            team: platform
          annotations:
            summary: "ðŸš¨ All backends down for {{ $labels.subdomain }}"
            description: "Service {{ $labels.subdomain }} has NO healthy backends for 10+ minutes. Returning 503 errors."
            action: "Check backend health, network connectivity, and health check configuration."

    # ===== DAILY SUMMARY ALERTS (BATCH NOTIFICATIONS) =====
    - name: taiji.daily-summary
      interval: 5m
      rules:
        # Daily backend health summary - fires once per day if any backend is consistently bad
        - alert: TaijiBackendHealthSummary
          expr: |
            (
              sum by (subdomain, backend) (rate(proxy_requests_total{job="taiji",environment="production",status_code=~"5.."}[24h]))
              /
              sum by (subdomain, backend) (rate(proxy_requests_total{job="taiji",environment="production"}[24h]))
            ) > 0.05
            and
            sum by (subdomain, backend) (rate(proxy_requests_total{job="taiji",environment="production"}[24h])) > 0.01
          for: 1h
          labels:
            severity: info
            service: taiji
            team: platform
            daily_summary: "true"
          annotations:
            summary: "Daily: Backend {{ $labels.backend }} has {{ $value | humanizePercentage }} error rate"
            description: "Backend {{ $labels.backend }} for {{ $labels.subdomain }} had {{ $value | humanizePercentage }} errors over 24h."

        # Latency summary - only alert if P99 is consistently high
        - alert: TaijiHighLatencySummary
          expr: |
            histogram_quantile(0.99,
              sum by (le, subdomain) (rate(proxy_request_duration_seconds_bucket{job="taiji",environment="production"}[24h]))
            ) > 2
          for: 2h
          labels:
            severity: info
            service: taiji
            team: platform
            daily_summary: "true"
          annotations:
            summary: "Daily: Subdomain {{ $labels.subdomain }} has high latency"
            description: "P99 latency is {{ printf \"%.2f\" $value }}s over 24h period."

  # ===== REDUCED NOISE: Removed/Modified Alerts =====
  # The following alerts were removed to reduce noise:
  # - TaijiSubdomainHighServerErrors (too sensitive)
  # - TaijiSubdomainElevatedServerErrors (too sensitive)
  # - TaijiBackendHighErrorRate (replaced by service-level alert)
  # - TaijiBackendMany502Errors (grouped into service degradation)
  # - TaijiBackendHighLatency (moved to daily summary)
  # - TaijiBackendSlowResponse (moved to daily summary)
  # - Individual backend alerts (use daily summary instead)
  #
  # Strategy:
  # - CRITICAL: Only fires for true outages or severe degradation (>10% errors for 15min)
  # - WARNING: Fires for sustained issues (>20% errors for 20min, per subdomain)
  # - INFO: Daily summaries for trends and minor issues
  # - Longer "for" durations prevent flapping
  # - Aggregate metrics reduce per-backend noise
